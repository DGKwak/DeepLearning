{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_IN_PATH = './data/sample_data/data/'\n",
    "train_data = pd.read_csv(DATA_IN_PATH+'/train.csv', header = 0)\n",
    "test_data = pd.read_csv(DATA_IN_PATH+'/test.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train_data, test_data]\n",
    "\n",
    "label_mapping = {1 : 0, 2 : 0, 3 : 0, 4 : 1, 5 : 1}\n",
    "\n",
    "for dataset in train_test_data:\n",
    "    dataset['label'] = dataset['rating'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 함수 만들기\n",
    "def preprocessing(review, okt, remove_stopwords = False, stop_words =[]):\n",
    "  #함수인자설명\n",
    "  # review: 전처리할 텍스트\n",
    "  # okt: okt객체를 반복적으로 생성하지 않고 미리 생성 후 인자로 받음\n",
    "  # remove_stopword: 불용어를 제거할지 여부 선택. 기본값 False\n",
    "  # stop_words: 불용어 사전은 사용자가 직접 입력, 기본값 빈 리스트\n",
    "\n",
    "  # 1. 한글 및 공백 제외한 문자 모두 제거\n",
    "  review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]','',review)\n",
    "  \n",
    "  #2. okt 객체를 활용하여 형태소 단어로 나눔\n",
    "  word_review = okt.morphs(review_text,stem=True)\n",
    "\n",
    "  if remove_stopwords:\n",
    "    #3. 불용어 제거(선택)\n",
    "    word_review = [token for token in word_review if not token in stop_words]\n",
    "  return word_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['입출금',\n",
       "  '통장',\n",
       "  '만들다',\n",
       "  '때',\n",
       "  '인증',\n",
       "  '방법',\n",
       "  '을',\n",
       "  '바꾸다',\n",
       "  '하다',\n",
       "  '이전',\n",
       "  '으로',\n",
       "  '가다',\n",
       "  '버튼',\n",
       "  '없다',\n",
       "  '처음',\n",
       "  '부터',\n",
       "  '다시',\n",
       "  '만들다',\n",
       "  '하다',\n",
       "  '이미',\n",
       "  '계좌',\n",
       "  '개설',\n",
       "  '진행',\n",
       "  '중이',\n",
       "  '라서',\n",
       "  '다시',\n",
       "  '들어가다',\n",
       "  '셀카',\n",
       "  '인증',\n",
       "  '으로',\n",
       "  '돼다',\n",
       "  '다시',\n",
       "  '만들다',\n",
       "  '일',\n",
       "  '기다리다',\n",
       "  '하다',\n",
       "  '빼다',\n",
       "  '다',\n",
       "  '좋다',\n",
       "  '선택',\n",
       "  '수정',\n",
       "  '게',\n",
       "  '끄다',\n",
       "  '해주다',\n",
       "  '하다'],\n",
       " ['자꾸',\n",
       "  '핸드폰',\n",
       "  '인증',\n",
       "  '라고',\n",
       "  '반복',\n",
       "  '되다',\n",
       "  '그때',\n",
       "  '마다',\n",
       "  '매번',\n",
       "  '인증',\n",
       "  '하다',\n",
       "  '하다',\n",
       "  '번거롭다',\n",
       "  '기도',\n",
       "  '하고',\n",
       "  '너무',\n",
       "  '짜증나다',\n",
       "  '앱',\n",
       "  '을',\n",
       "  '새롭다',\n",
       "  '깔다',\n",
       "  '안내',\n",
       "  '를',\n",
       "  '받다',\n",
       "  '새롭다',\n",
       "  '깔다',\n",
       "  '마찬가지',\n",
       "  '이고',\n",
       "  '왜',\n",
       "  '갑자기',\n",
       "  '이렇다',\n",
       "  '현상',\n",
       "  '계속',\n",
       "  '되다',\n",
       "  '기술',\n",
       "  '팀',\n",
       "  '에서',\n",
       "  '조속',\n",
       "  '히',\n",
       "  '해결',\n",
       "  '해주다',\n",
       "  '바라다',\n",
       "  '핸드폰',\n",
       "  '몇',\n",
       "  '년',\n",
       "  '계속',\n",
       "  '사용',\n",
       "  '하고',\n",
       "  '있다',\n",
       "  '기',\n",
       "  '기',\n",
       "  '변경',\n",
       "  '라는',\n",
       "  '게',\n",
       "  '납득',\n",
       "  '안되다'],\n",
       " ['전체',\n",
       "  '적',\n",
       "  '으로',\n",
       "  '편리하다',\n",
       "  '좋다',\n",
       "  '이용',\n",
       "  '하다',\n",
       "  '줄',\n",
       "  '만',\n",
       "  '안다',\n",
       "  '대체로',\n",
       "  '스스로',\n",
       "  '웬만하다',\n",
       "  '건',\n",
       "  '처리',\n",
       "  '하다',\n",
       "  '있다',\n",
       "  '하다',\n",
       "  '무엇',\n",
       "  '보다',\n",
       "  '스마트',\n",
       "  '출금',\n",
       "  '을',\n",
       "  '한정',\n",
       "  '적',\n",
       "  '이지만',\n",
       "  '편의점',\n",
       "  '자동화',\n",
       "  '기기',\n",
       "  '에서',\n",
       "  '수수료',\n",
       "  '면제',\n",
       "  '로',\n",
       "  '이용',\n",
       "  '하다',\n",
       "  '있다',\n",
       "  '점도',\n",
       "  '정말',\n",
       "  '좋다',\n",
       "  '다만',\n",
       "  '발급',\n",
       "  '관련',\n",
       "  '하다',\n",
       "  '정말',\n",
       "  '급하다',\n",
       "  '건',\n",
       "  '을',\n",
       "  '기다리다',\n",
       "  '한다는',\n",
       "  '치명',\n",
       "  '적',\n",
       "  '라면',\n",
       "  '치명',\n",
       "  '적',\n",
       "  '으로',\n",
       "  '느껴지다',\n",
       "  '때',\n",
       "  '있다'],\n",
       " ['편리하다',\n",
       "  '자다',\n",
       "  '사용',\n",
       "  '하고',\n",
       "  '있다',\n",
       "  '만족스럽다',\n",
       "  '그래도',\n",
       "  '개선',\n",
       "  '하다',\n",
       "  '좋다',\n",
       "  '같다',\n",
       "  '을',\n",
       "  '적다',\n",
       "  '보다',\n",
       "  '우선',\n",
       "  '첫',\n",
       "  '화면',\n",
       "  '에서',\n",
       "  '통장',\n",
       "  '순서',\n",
       "  '를',\n",
       "  '직접',\n",
       "  '정',\n",
       "  '하다',\n",
       "  '있다',\n",
       "  '좋다',\n",
       "  '모임',\n",
       "  '통장',\n",
       "  '같다',\n",
       "  '밑',\n",
       "  '으로',\n",
       "  '내리다',\n",
       "  '제',\n",
       "  '모으다',\n",
       "  '있다',\n",
       "  '통장',\n",
       "  '이나',\n",
       "  '적금',\n",
       "  '을',\n",
       "  '위',\n",
       "  '에서',\n",
       "  '보다',\n",
       "  '그리고',\n",
       "  '자산',\n",
       "  '액수',\n",
       "  '숨기다',\n",
       "  '기능',\n",
       "  '있다',\n",
       "  '좋다',\n",
       "  '되다',\n",
       "  '이면',\n",
       "  '첫',\n",
       "  '화면',\n",
       "  '에서',\n",
       "  '간단하다',\n",
       "  '버튼',\n",
       "  '하나로',\n",
       "  '숨기다',\n",
       "  '보이다',\n",
       "  '하다',\n",
       "  '좋다',\n",
       "  '감사하다']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 텍스트 전처리\n",
    "stop_words = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한','카뱅','도']\n",
    "okt = Okt()\n",
    "clean_train_review = []\n",
    "\n",
    "for review in train_data['content']:\n",
    "  # 리뷰가 문자열인 경우만 전처리 진행\n",
    "  if type(review) == str:\n",
    "    clean_train_review.append(preprocessing(review,okt,remove_stopwords=True,stop_words= stop_words))\n",
    "  else:\n",
    "    clean_train_review.append([]) #str이 아닌 행은 빈칸으로 놔두기\n",
    "\n",
    "clean_train_review[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clean_train_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 리뷰도 동일하게 전처리\n",
    "clean_test_review = []\n",
    "for review in test_data['content']:\n",
    "  if type(review) == str:\n",
    "    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
    "  else:\n",
    "    clean_test_review.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 벡터 변환 후 일정 길이 넘어가거나 모자라는 리뷰 패딩처리\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index #단어사전형태\n",
    "MAX_SEQUENCE_LENGTH = 8 #문장 최대 길이\n",
    "\n",
    "#학습 데이터\n",
    "train_inputs = pad_sequences(train_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "\n",
    "#학습 데이터 라벨 벡터화\n",
    "train_labels = np.array(train_data['label'])\n",
    "\n",
    "#평가 데이터 \n",
    "test_inputs = pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "#평가 데이터 라벨 벡터화\n",
    "test_labels = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 8377\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 4916\n",
      "단어 집합에서 희귀 단어의 비율: 58.684493255342005\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.4515907009550464\n",
      "단어 집합의 크기 : 3463\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import encode\n",
    "import encodings\n",
    "\n",
    "\n",
    "DEFAULT_PATH  = './data/sample_data/'\n",
    "DATA_PATH = 'clean_data/'\n",
    "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
    "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
    "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "data_configs={}\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) + 1\n",
    "\n",
    "#전처리한 데이터들 파일로저장\n",
    "if not os.path.exists(DEFAULT_PATH + DATA_PATH):\n",
    "  os.makedirs(DEFAULT_PATH+DATA_PATH)\n",
    "\n",
    "#전처리 학습데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_INPUT_DATA,'wb'),train_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_LABEL_DATA,'wb'),train_labels)\n",
    "#전처리 테스트데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_INPUT_DATA,'wb'),test_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_LABEL_DATA,'wb'),test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 데이터 불러오기\n",
    "DATA_PATH = './data/sample_data/clean_data/'\n",
    "DATA_OUT = './data/sample_data/out/'\n",
    "INPUT_TRAIN_DATA = 'nsmc_train_input.npy'\n",
    "LABEL_TRAIN_DATA = 'nsmc_train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "train_input = np.load(open(DATA_PATH + INPUT_TRAIN_DATA,'rb'))\n",
    "train_input = pad_sequences(train_input,maxlen=train_input.shape[1])\n",
    "train_label = np.load(open(DATA_PATH + LABEL_TRAIN_DATA,'rb'))\n",
    "prepro_configs = data_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= 'cnn_classifier_kr'\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kargs={'model_name': model_name, 'vocab_size':prepro_configs['vocab_size'],'embbeding_size':128, 'num_filters':100,'dropout_rate':0.5, 'hidden_dimension':250,'output_dimension':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, **kargs):\n",
    "    super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "    self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])\n",
    "    self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid',activation = tf.keras.activations.relu,\n",
    "                                    kernel_constraint = tf.keras.constraints.MaxNorm(max_value=3)) for kernel_size in [3,4,5]]\n",
    "    self.pooling = layers.GlobalMaxPooling1D()\n",
    "    self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
    "    self.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
    "                            activation = tf.keras.activations.relu,\n",
    "                            kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "    self.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
    "                            activation=tf.keras.activations.sigmoid,\n",
    "                            kernel_constraint= tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "    \n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.embedding(x)\n",
    "    x = self.dropout(x)\n",
    "    x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis = 1)\n",
    "    x = self.fc1(x)\n",
    "    x = self.fc2(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sample_data/out/cnn_classifier_kr -- Folder already exists \n",
      "\n",
      "Epoch 1/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4861 - accuracy: 0.7472\n",
      "Epoch 1: val_accuracy improved from -inf to 0.80510, saving model to ./data/sample_data/out/cnn_classifier_kr\\weights.h5\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.4862 - accuracy: 0.7472 - val_loss: 0.4287 - val_accuracy: 0.8051\n",
      "Epoch 2/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8899\n",
      "Epoch 2: val_accuracy improved from 0.80510 to 0.87932, saving model to ./data/sample_data/out/cnn_classifier_kr\\weights.h5\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.2854 - accuracy: 0.8899 - val_loss: 0.2754 - val_accuracy: 0.8793\n",
      "Epoch 3/10\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.2265 - accuracy: 0.9154\n",
      "Epoch 3: val_accuracy improved from 0.87932 to 0.89122, saving model to ./data/sample_data/out/cnn_classifier_kr\\weights.h5\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.2276 - accuracy: 0.9151 - val_loss: 0.2678 - val_accuracy: 0.8912\n",
      "Epoch 4/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9302\n",
      "Epoch 4: val_accuracy improved from 0.89122 to 0.89348, saving model to ./data/sample_data/out/cnn_classifier_kr\\weights.h5\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.1979 - accuracy: 0.9302 - val_loss: 0.2655 - val_accuracy: 0.8935\n",
      "Epoch 5/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.1740 - accuracy: 0.9405\n",
      "Epoch 5: val_accuracy improved from 0.89348 to 0.89632, saving model to ./data/sample_data/out/cnn_classifier_kr\\weights.h5\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.1740 - accuracy: 0.9405 - val_loss: 0.2733 - val_accuracy: 0.8963\n",
      "Epoch 6/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.1550 - accuracy: 0.9469\n",
      "Epoch 6: val_accuracy did not improve from 0.89632\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.1550 - accuracy: 0.9469 - val_loss: 0.2837 - val_accuracy: 0.8963\n",
      "Epoch 7/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.9550\n",
      "Epoch 7: val_accuracy did not improve from 0.89632\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.1371 - accuracy: 0.9550 - val_loss: 0.3048 - val_accuracy: 0.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/sample_data/my_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/sample_data/my_models/assets\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier(**kargs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy')])\n",
    "\n",
    "#검증 정확도를 통한 EarlyStopping 기능 및 모델 저장 방식 지정\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
    "checkpoint_path = DATA_OUT + model_name +'\\weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "  print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "  print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor = 'val_accuracy', verbose=1, save_best_only = True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs = NUM_EPOCHS,\n",
    "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "# 모델 저장하기\n",
    "save_model(model,'./data/sample_data/my_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEST_DATA = 'nsmc_test_input.npy'\n",
    "LABEL_TEST_DATA = 'nsmc_test_label.npy'\n",
    "SAVE_FILE_NM = 'weights.h5'\n",
    "\n",
    "test_input = np.load(open(DATA_PATH+INPUT_TEST_DATA,'rb'))\n",
    "test_input = pad_sequences(test_input,maxlen=test_input.shape[1])\n",
    "test_label_data = np.load(open(DATA_PATH + LABEL_TEST_DATA, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 1s 1ms/step - loss: 0.1541 - accuracy: 0.9503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15410929918289185, 0.9502947330474854]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./data/sample_data/out/cnn_classifier_kr\\weights.h5')\n",
    "model.evaluate(test_input, test_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "okt = Okt()\n",
    "tokenizer  = Tokenizer()\n",
    "\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "prepro_configs = data_configs\n",
    "prepro_configs['vocab'] = word_vocab\n",
    "\n",
    "tokenizer.fit_on_texts(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(sentence):\n",
    "    MAX_LENGTH = 8 #문장최대길이\n",
    "    \n",
    "    sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
    "    stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한','카뱅','도'] # 불용어 추가할 것이 있으면 이곳에 추가\n",
    "    sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
    "    vector  = tokenizer.texts_to_sequences(sentence)\n",
    "    pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
    "    #print('변환된 문자 결과값: ',pad_new)\n",
    "    model.load_weights('./data/sample_data/out/cnn_classifier_kr\\weights.h5') #모델 불러오기\n",
    "    predictions = model.predict(pad_new)\n",
    "    predictions = float(predictions.squeeze(-1)[1])\n",
    "    #print(predictions)\n",
    "    if(predictions > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(predictions * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - predictions) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "78.21% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이체하기 편하고 쉬워서 좋아요ㅎㅎ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f6e3b18aee0be1d8bbda429ec202ef0084c30d3267feb3deeb863225f680e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
